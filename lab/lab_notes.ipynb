{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "### b)\n",
    "\n",
    "I choose the size of 32x32 because of two reasons\n",
    "\n",
    "#### Reason 1 \n",
    "\n",
    "[When I did a small eda](https://github.com/felix-tjernberg/ai21-deep-learning/blob/main/lab/1_image_processing.ipynb) I found out that the smallest side in the dataset was 32px \n",
    "\n",
    "I decided at that point to scale down everything to 32x32px so all the files have the same amount of data.\n",
    "\n",
    "[After doing a lot of hyper parameter tuning](https://github.com/felix-tjernberg/ai21-deep-learning/blob/main/lab/2_32x32_models.ipynb) I realized that 32x32px was to little information to get a really good prediction, I maxed out at around .73 val_acc which isn't very good. \n",
    "\n",
    "One thing that was notable for this image size was that a smaller kernel size of 2x2 gave a somewhat better result at .74, which might be suggest that giving the model got some more information with a smaller stride and kernel size\n",
    "\n",
    "#### Reason 2\n",
    "\n",
    "I also wanted to see how little information you could give to a model to get a decent inference\n",
    "\n",
    "[Also tried 64x64px](https://github.com/felix-tjernberg/ai21-deep-learning/blob/main/lab/2_64x64_models.ipynb) but it did not really make much of a difference, something I also tried during this time was to see how much difference the transformations helped. It seemed to do quite a lot for this image size, this theory was also reinforced when talking to a classmate as they dit not have so much of a jump in performance from transformations\n",
    "\n",
    "## d)\n",
    "\n",
    "I choose the parameters for augmentations quite arbitrarily\n",
    "\n",
    "Something I did realize when talking to classmates is that havning a rotation range of up to 90 degrees might be quite high as they seemed to get worse models when having that high of degree rotation. \n",
    "\n",
    "So I'm quite curious what my models learned as they seemed to like the high rotation range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "### a)\n",
    "\n",
    "[When I tried to do the hyperparameter tune my first models](https://github.com/felix-tjernberg/ai21-deep-learning/blob/main/lab/2_32x32_models.ipynb) I tried to follow the rules of thumb from [this article](https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af), in my case hyper parameter tuning did not help much: which might be because the model did not get enough information from the beginning _Shit in shit out_\n",
    "\n",
    "I did talk to my teacher and he said that hyperparameter tuning doesn't usually give you more than a few more percent in extra performance so long as you have a decent network architecture from the beginning\n",
    "\n",
    "So my take away from this lab is that hyperparameter tuning of a deep learning model should only be done a small amount. If you still have a bad result you should probably start rethinking if your data in is good enough for your expected result or if you have really chosen the correct type of model: this thought was also reinforced when doing transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('ai21-deep-learning-WGFYuZhT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ab85649342dae147ebae0a60e5dbaca3ea59d0f56a6a7299a1dc242a894ae53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
