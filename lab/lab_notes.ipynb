{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "### b)\n",
    "\n",
    "I choose the size of 32x32 because of two reasons\n",
    "\n",
    "#### Reason 1 \n",
    "\n",
    "[When I did a small eda](https://github.com/felix-tjernberg/ai21-deep-learning/blob/main/lab/1_image_processing.ipynb) I found out that the smallest side in the dataset was 32px \n",
    "\n",
    "I decided at that point to scale down everything to 32x32px so all the files have the same amount of data.\n",
    "\n",
    "[After doing a lot of hyper parameter tuning](https://github.com/felix-tjernberg/ai21-deep-learning/blob/main/lab/2_32x32_models.ipynb) I realized that 32x32px was to little information to get a really good prediction, I maxed out at around .73 val_acc which isn't very good. \n",
    "\n",
    "One thing that was notable for this image size was that a smaller kernel size of 2x2 gave a somewhat better result at .74, which might be suggest that giving the model got some more information with a smaller stride and kernel size\n",
    "\n",
    "#### Reason 2\n",
    "\n",
    "I also wanted to see how little information you could give to a model to get a decent inference\n",
    "\n",
    "[Also tried 64x64px](https://github.com/felix-tjernberg/ai21-deep-learning/blob/main/lab/2_64x64_models.ipynb) but it did not really make much of a difference, something I also tried during this time was to see how much difference the transformations helped. It seemed to do quite a lot for this image size, this theory was also reinforced when talking to a classmate as they dit not have so much of a jump in performance from transformations\n",
    "\n",
    "## d)\n",
    "\n",
    "I choose the parameters for augmentations quite arbitrarily\n",
    "\n",
    "Something I did realize when talking to classmates is that havning a rotation range of up to 90 degrees might be quite high as they seemed to get worse models when having that high of degree rotation. \n",
    "\n",
    "So I'm quite curious what my models learned as they seemed to like the high rotation range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "### a)\n",
    "\n",
    "[When I tried to do the hyperparameter tune my first models](https://github.com/felix-tjernberg/ai21-deep-learning/blob/main/lab/2_32x32_models.ipynb) I tried to follow the rules of thumb from [this article](https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af), in my case hyper parameter tuning did not help much: which might be because the model did not get enough information from the beginning _Shit in shit out_\n",
    "\n",
    "I did talk to my teacher and he said that hyperparameter tuning doesn't usually give you more than a few more percent in extra performance so long as you have a decent network architecture from the beginning\n",
    "\n",
    "So my take away from this lab is that hyperparameter tuning of a deep learning model should only be done a small amount. If you still have a bad result you should probably start rethinking if your data in is good enough for your expected result or if you have really chosen the correct type of model: this thought was also reinforced when doing transfer learning\n",
    "\n",
    "If I did this lab again I would do more hyperparameter tuning on the image augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose the 64x64 model as it seemed to do a little bit better than all the 32x32 models\n",
    "\n",
    "Choose training 137 epochs as it's a cool number and around that point the [64x64px model got score 0.75 in val_acc](https://github.com/felix-tjernberg/ai21-deep-learning/blob/main/lab/2_64x64_models.ipynb)\n",
    "\n",
    "Getting 70% accuracy is actually quite decent for a model that only takes images that is 64x64px large :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For transfer model I choose Xception as feature extractor because it seemed to be a big boi :)\n",
    "\n",
    "For weight I used imagenet, after evaluation I started to wonder if imagenet includes the images from the dataset we were provided with in this lab\n",
    "\n",
    "Anyways the exercise was to put a MLP model behind the Xception, [which I tried and got very good results with (0.99.4 % val_acc)](https://github.com/felix-tjernberg/ai21-deep-learning/blob/main/lab/2_transference_feature_map_method.ipynb)\n",
    "\n",
    "But a classmate is supersold on RandomForestClassifier when it comes to classification and he got 97% accuracy on data that wasn't augmented, so I tried on my augmented validation/train data and got 99% accuracy!\n",
    "\n",
    "This then became my choice for training on train and validation data: Xception as feature extractor and RandomForestClassifier as classifier\n",
    "\n",
    "[And the results speaks for itself](https://github.com/felix-tjernberg/ai21-deep-learning/blob/main/lab/2_model_selection_and_inference.ipynb), only 8 misclassified images!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('ai21-deep-learning-WGFYuZhT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ab85649342dae147ebae0a60e5dbaca3ea59d0f56a6a7299a1dc242a894ae53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
